![Katacoda Logo](./assets/logo-text-with-head.png)

В этой лабораторной работе мы расскажем о концепции **ReplicaSets**, **Deployments** и предложим нам создать несколько **Deployments**, используя разные стратегии.

# Управляющие циклы


### Контроллер **Deployments**

Как происходит настройка топологии ресурсов, используемых в **Kubernetes**. Если задуматься, то **Deployment** — это в действительности просто коллекция **ReplicaSets**, а **ReplicaSet** — коллекция подов. Что же происходит в **Kubernetes** для создания этой иерархии из одного **HTTP**-запроса? Здесь берутся за дело встроенные контроллеры **K8s**.

**Kubernetes** активно использует «контроллеры» повсюду в своей системе. Контроллер — это асинхронный скрипт, сверяющий текущее состояние системы **Kubernetes** с желаемым. Каждый контроллер отвечает за свою небольшую часть и запускается компонентом **kube-controller-manager**. Давайте представим себя первому из них, кто вступает в дело, — контроллеру **Deployment**.

После того, как запись с **Deployment** сохранена в **etcd** и инициализирована, он становится видимой в **kube-apiserver**. Когда появляется новый ресурс, его обнаруживает контроллер **Deployment**, в задачи которого входит отслеживание изменений среди соответствующих записей (**Deployments**). В нашем случае контроллер регистрирует специальный [**callback** для контролера **Deployment**](hhttps://github.com/kubernetes/kubernetes/blob/a701a42a82da8c3dec18cb35124ee9038c91cca6/pkg/controller/deployment/deployment_controller.go#L122) для событий создания через информатор (подробности о том, что это такое, смотрите ниже).

Этот обработчик будет вызван, когда Deployment впервые станет доступным, и начнёт свою работу с [добавления объекта во внутреннюю очередь](https://github.com/kubernetes/kubernetes/blob/a701a42a82da8c3dec18cb35124ee9038c91cca6/pkg/controller/deployment/deployment_controller.go#L170). 

К тому времени, когда он дойдёт до обработки этого объекта, [контроллер проинспектирует](https://github.com/kubernetes/kubernetes/blob/a701a42a82da8c3dec18cb35124ee9038c91cca6/pkg/controller/deployment/deployment_controller.go#L572) **Deployment** и [поймёт](https://github.com/kubernetes/kubernetes/blob/a701a42a82da8c3dec18cb35124ee9038c91cca6/pkg/controller/deployment/deployment_controller.go#L633), что нет связанных с ним записей **ReplicaSet** и подов. Эту информацию он получает, опрашивая **kube-apiserver** по **label selectors** (подробнее о них читайте в [документации Kubernetes](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors).). Интересно заметить, что этот процесс синхронизации ничего не знает о состоянии (является **state agnostic**): он проверяет новые записи точно так же, как и уже существующие.

Узнав, что нужных записей не существует, [контроллер начинает процесс масштабирования](https://github.com/kubernetes/kubernetes/blob/b694d518428ac655780d812f7dd4cf72d3e24763/pkg/controller/deployment/sync.go#L385), чтобы прийти к ожидаемому состоянию. Этот процесс осуществляется с помощью выкатывания (например, создания) ресурса **ReplicaSet**, назначая ему **label selector** и присваивая первую ревизию. **PodSpec** и другие метаданные для **ReplicaSet** копируются из манифеста **Deployment**. Иногда после этого может потребоваться также обновить запись **Deployment** (например, если установлен progress deadline; т.е. [определено поле спецификации .spec.progressDeadlineSeconds](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progress-deadline-seconds).).

[После этого статус обновляется](https://github.com/kubernetes/kubernetes/blob/b694d518428ac655780d812f7dd4cf72d3e24763/pkg/controller/deployment/sync.go#L70) и начинается тот же самый цикл сверки, сравнивающий **Deployment** с желаемым, законченным состоянием. Поскольку контроллер знает только о создании **ReplicaSets**, этап сверки продолжается следующим контроллером, ответственным за **ReplicaSets**.

### Контроллер **ReplicaSets**

На предыдущем этапе контроллер **Deployments** создал первый **ReplicaSet** для **Deployment**, однако у нас до сих пор нет подов. Здесь-то и приходит на помощь контроллер **ReplicaSets**. Его задача — следить за жизненным циклом **ReplicaSets** и зависимых ресурсов (подов). Как и у большинства других контроллеров, это происходит благодаря обработчикам триггеров определённых событий.

Событие, в котором мы заинтересованы, — создание. Когда создаётся **ReplicaSet** (в результате деятельности контроллера **Deployments**), [контроллер **RS** инспектирует состояние нового **ReplicaSet** и понимает](https://github.com/kubernetes/kubernetes/blob/3e315aa0f8ec0034109b278ef5860dd6e8a5bc21/pkg/controller/replicaset/replica_set.go#L583), что есть разница между тем, что существует, и тем, что требуется. Поэтому он корректирует состояние, выкатывая поды, которые будут принадлежать **ReplicaSet**. Процесс их создания происходит аккуратно, соответствуя числу всплесков **ReplicaSet** (унаследованных от родительского **Deployment**).

Операции создания для подов тоже выполняются пачками, начиная со **SlowStartInitialBatchSize** и увеличивая это значение вдвое при каждой успешной итерации операции «медленного старта». Такой подход призван снизить риск забрасывания **kube-apiserver** ненужными **HTTP**-запросами в случае частых ошибок загрузки подов (например, из-за квот на ресурсы). Если у нас ничего не получится, то пусть это произойдёт с минимальным влиянием на остальные компоненты системы.

Kubernetes реализует иерархию объектов через ссылки на владельца, **Owner References** (поле в дочернем ресурсе, ссылающееся на **ID** родителя). Это не только гарантирует, что сборщик мусора найдёт все дочерние ресурсы при удалении ресурса, управляемого контроллером (каскадное удаление), но и обеспечивает эффективный способ для родительских ресурсов не бороться за своих детей (представьте себе сценарий, в котором два потенциальных родителя думают, что владеют одним и тем же ребёнком).

Ещё одно преимущество архитектуры с **Owner References** — она является **stateful**: если любой контроллер должен перезагрузиться, его простой не затронет другие части системы, поскольку топология ресурсов не зависит от контроллера. Ориентация на изоляцию проникла и в архитектуру самих контроллеров: они не должны работать с ресурсами, которыми не владеют явным образом. Наоборот, контроллеры должны быть избирательными в своих притязаниях на владение ресурсами, не вмешивающимися (**non-interfering**) и не разделяющими (**non-sharing**).

Но вернёмся к ссылкам на владельца. Иногда в системе появляются «осиротевшие» ресурсы — обычно это происходит из-за того, что:

 >   удаляется родитель, но не его дети;
 >   политики сбора мусора запрещают удаление ребёнка.


Когда такое происходит, контроллеры проверяют, что сироты были приняты новым родителем. Множество родителей могут претендовать на ребёнка, но только один из них добьётся успеха (остальные получат ошибку валидации).

[«What happens when… Kubernetes edition!» и написанный Jamie Hannaford из компании Rackspace](https://github.com/jamiehannaford/what-happens-when-k8s)

[Перевод: «What happens when… Kubernetes edition!» и написанный Jamie Hannaford из компании Rackspace, является отличной иллюстрацией работы многих механизмов Kubernetes, Что происходит в Kubernetes при запуске kubectl run? Часть 1](https://habr.com/en/company/flant/blog/342658/)

[Перевод: «What happens when… Kubernetes edition!» и написанный Jamie Hannaford из компании Rackspace, является отличной иллюстрацией работы многих механизмов Kubernetes,Что происходит в Kubernetes при запуске kubectl run? Часть 2](https://habr.com/en/company/flant/blog/342822/)

Для получения дополнительной информации  [documentation][docs].

<!-- Links Referenced -->

[docs]:           https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
