In this step, we will simulate failure by cordoning the node where Minio is running and then deleting the Minio pod. The pod will then be rescheduled by the [STorage ORchestrator for Kubernetes (STORK)](https://github.com/libopenstorage/stork/) to make sure it lands on one of the nodes that has of replica of the data.

### Step: Simulate a node failure to force Minio to restart

Cordon the node where Minio is running to simulate a node failure or network partition:
```
NODE=`kubectl get pods -o wide | grep px-minio | awk '{print $7}'`
kubectl cordon ${NODE}
```{{execute T1}}

Then delete the Minio pod:
```
POD=$(kubectl get po | grep px-minio | awk '{print $1}')
kubectl delete pod $POD
```{{execute T1}}

Once the Minio pod gets deleted, STORK will work with Kubernetes to start a new Minio pod on another node where the data has been replicated.

### Step: Verify replacement pod starts running

Below commands wait till the new Minio pod is ready.
```
watch kubectl get pods -o wide
```{{execute T1}}

When the pod come back up it will be in the Running state. When it does hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

Before you proceed you should uncordon your node:
```
kubectl uncordon ${NODE}
```{{execute T1}}

Now that we have the new Minio pod running, let's check if the database we previously created is still intact.
