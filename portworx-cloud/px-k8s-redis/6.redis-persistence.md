In this step, we will simulate failure by cordoning the node where Redis is running and then deleting the Redis pod. The pod will then be resheduled by the [STorage ORchestrator for Kubernetes (STORK)](https://github.com/libopenstorage/stork/) to make sure it lands on one of the nodes that has of replica of the data.

### Step: Simulate a node failure to force Redis to restart

First we will cordon the node where Redis is running to simulate a node failure or network partition:
```
NODE=`kubectl get pods -l app=redis -o wide | grep -v NAME | awk '{print $7}'`
kubectl cordon ${NODE}
```{{execute T1}}


See that the node is no longer eligible for scheduling:
```
kubectl get nodes -o wide
```{{execute T1}}

Then delete the Redis pod.
Once the redis pod gets deleted, Kubernetes will start to create a new redis pod on another node.
```
POD=`kubectl get pods -l app=redis -o wide | grep -v NAME | awk '{print $1}'`
kubectl delete pod ${POD}
watch kubectl get pods -l app=redis -o wide
```{{execute T1}}


### Step: Verify 

When the pod come back up it will be in the Running state. When it does hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

Before you proceed you should uncordon your node:
```
kubectl uncordon ${NODE}
```{{execute T1}}

Now that we have the new redis pod running, let's check if the data we previously created is still intact.
