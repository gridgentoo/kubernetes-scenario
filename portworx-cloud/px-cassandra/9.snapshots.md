In this step, we will take a snapshot of all volumes for our Cassandra cluster, then drop our database table.

### Step: Take snapshot using kubectl

First let's insert a new record in our features table so we can show that the snapshot will take the latest available data:
```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
INSERT INTO portworx.features (id, name, value) VALUES ('px-6', '3DSnaps', 'Application/Cluster aware snapshots!');
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

We're going to use STORK to take a 3DSnapshot of our Cassandra cluster. Take a look at the px-snap.yaml file ```cat px-snap.yaml ```{{execute T1}} and notice that we are going to force a ```nodetool flush``` command on each cluster member before we take the snapshot. As explained before, that will force all data to be written to disk in order to ensure consistency of the snapshot. We also defined the volume group name (cassandra_vg) so Portworx will synchronously quiesce I/O on all volumes before triggering their snapshots.

Now let's take a snapshot.
```
kubectl create -f px-snap.yaml
```{{execute T1}}

You can see the snapshots using the following command:
```
kubectl get volumesnapshot,volumesnapshotdatas
```{{execute T1}}

### Step: Drop features table

Now we're going to go ahead and do something stupid because it's Katacoda and we're here to learn.

```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
DROP TABLE IF EXISTS portworx.features;
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

Ok, so we deleted our database, what now? Restore your snapshot and cary on.
