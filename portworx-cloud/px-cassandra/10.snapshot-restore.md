In this step, we will check the state of our sample Cassandra database.


### Step: Restore the snapshot and see your data is still there

First we will scale the stateful set to 0 nodes so we can restore the snapshots.
```
kubectl scale sts cassandra --replicas=0
```{{execute T1}}

Wait for the pods to be deleted:
```
watch kubectl get pods
```{{execute T1}}

Once the cassandra pods have been deleted you can ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

Now let's open a shell in one of our Portworx containers:
```
PX_POD=$(kubectl get pods -l name=portworx -n kube-system -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n kube-system -it $PX_POD bash
```{{execute T1}}

Then iterate through the group snapshot volumes and first delete the original volume before cloning the snapshot using the same name.
```
for pvc in `/opt/pwx/bin/pxctl v l | grep group | awk '{print $2}'`; \
  do /opt/pwx/bin/pxctl v d ${pvc:24:100} -f; /opt/pwx/bin/pxctl v clone --name ${pvc:24:100} $pvc; done
exit
```{{execute T1}}

Now we can scale our cluster back up:
```
kubectl scale --replicas=3 sts cassandra
```{{execute T1}}

Watch to see the cassandra pods come back with the restored volumes.
```
watch kubectl get pods
```{{execute T1}}

When the pod is in Running state then then hit ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

### Step: Verify data is still available

Start a CQL Shell session:
```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
```{{execute T1}}

Select rows from the keyspace we previously created:
```
SELECT id, name, value FROM portworx.features;
```{{execute T1}}

Now that we have verify our data survived the node failure let's ```quit```{{execute T1}} the cqlsh session.
