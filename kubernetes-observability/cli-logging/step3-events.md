Events capture the create, read, update, and delete ([CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete)) transitions for the cluster objects during their lifecycles. These are not your application events, on your application data plane. Instead, these are the journal of changes to the cluster, on the control plane. Kubernetes administrators are keenly interested in these log details to gain insights into cluster resource events.

`kubectl get events`{{execute}}

At the end of this listing, you can see events related to starting the application during the previous step.

The listing only reveals events associated with activities specific to the `default` namespace. To see events related to the cluster components specify the`kube-system` namespace.

`kubectl get events --namespace=kube-system`{{execute}}

These cluster events are stored in _etcd_ and managed by Kubernetes. Because these events accumulate, the older ones are automatically purged. The typical default is one hour, and there is a setting called _time-to-live_ that on some clusters can be adjusted through the `kube-apiserver --event-ttl`. This time is kept short because if too many events accumulate within the _time-to-live_ period, it's possible _etcd_ can overfill. Additional tools can stream these events to other channels such as Elasticsearch and persistent data stores.

In the list, there is a column labeled `OBJECT`. You can query for specific objects by name.

`kubectl get event --field-selector=involvedObject.name=random-logger`{{execute}}

The same events can be seen at the end of the object's description.

`kubectl describe deployment/random-logger`{{execute}}

## Event Router

There are many tools for routing events to other services. As an example, Heptio maintains a simple [Eventrouter](https://github.com/heptiolabs/eventrouter) service.

> The event router serves as an active watcher of event resource in the Kubernetes system, which takes those events and pushes them to a user-specified sync. This is useful for different purposes, but most notably long term behavioral analysis of your workloads running on your Kubernetes cluster.

Give it a try, install Eventrouter.

`kubectl create -f https://raw.githubusercontent.com/heptiolabs/eventrouter/master/yaml/eventrouter.yaml`{{execute}}

In a moment, inspect the Eventrouter log and to see the type of data it will sync to other sources (if the sync was to be configured).

`kubectl logs -f deployment/eventrouter -n kube-system --limit-bytes=2048`{{execute}}

## Deeper into Kubelet and Linux Weeds

The Kubelet is the primary “node agent” that runs on each node. It's not a container, it's a Linux process core to Kubernetes.

`ps -lC "kubelet"`{{execute}}

On this Katacoda cluster and on this node called `master`, kubelet is managed with systemd and can be listed.

`systemctl --no-pager status kubelet`{{execute}}

Sometimes there is additional information in the journal entries for Kubelets.

`journalctl --no-pager --lines=3 --unit kubelet`{{execute}}

On other clusters where systemd is not used, the Kubernetes components that make up the control plane will produce logs in /var/log on each cluster node. In this Katacoda instance, there are two directories for `pods` and `containers`.

`ls -ld /var/log/*/`{{execute}}

> [Root hog or die](https://en.wikipedia.org/wiki/Root_hog_or_die)

It's not often you want to be _rooting_ around in these logs across all the nodes in your cluster, but it may be something to keep in mind when diagnosing hard to find issues.

These are the log entries related to the cluster and generated by controllers on the Kubernetes control plane. These events are unrelated to application log events, which you will explore next.
